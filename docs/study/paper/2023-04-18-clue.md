---
layout: post
title:  "CLUE"
parent: Paper
grand_parent: Study
# nav_order: 2
permalink: /docs/study/paper/2023-04-18-clue
date:   2023-04-18 13:00:00 +0900
---
# Contrastive Learning User Encoder
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

## Paper
- [Scaling Law for Recommendation Models: Towards General-purpose User Representations](https://arxiv.org/abs/2111.11294)
- [Attachment](https://deview.kr/data/deview/session/attach/[126]%EC%96%B8%EC%96%B4+%EB%AA%A8%EB%8D%B8+%EA%B8%B0%EB%B0%98%EC%9D%98+%EB%B2%94%EC%9A%A9+%EC%9C%A0%EC%A0%80+%EC%9E%84%EB%B2%A0%EB%94%A9%EA%B3%BC+%EC%9D%B4%EB%A5%BC+%ED%99%9C%EC%9A%A9%ED%95%9C+%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C+%EB%B0%8F+%EA%B4%91%EA%B3%A0+%ED%83%80%EA%B2%9F%ED%8C%85.pdf)

## Abstract
사용자를 Encoding하는 범용적 대규모 모델을 학습하자 ___training an universal user encoder at large scale___ = __CLUE__
- 거대 사전 학습 모델 `large-scale pretrained model` 과 범용 사용자 표현 `general-purpose user representation`

CLUE는
1. `task-agnostic` = task의 종류와 관계없이 적용 가능하다 - [_wikipedia_](https://en.wikipedia.org/wiki/Agnostic_(data))
2. `great transferability` = 다른 도메인 및 회사에 적용 가능하다
3. `impact of scale-factor` = `batch_size, train_data_size, seq_length` 과 같은 hyper-parameter에 따라 성능에 영향을 준다.

## Introduction
대규모 데이터로 사전학습된 모델 `(large-scale OR foundation model)` 중에서도 `범용성을 갖는, 사전학습된 User Representation`이 필요하다
1. __다양한 데이터 소스로 학습된 Representation이 좋은 전이 학습 능력을 보이는가__
    - CLUE는 두 개의 서비스에 대한 `multi-modal` User Embedding Space를 학습한다.
    - 각 서비스를 하나의 modality로 취급 _Considering each service as a modality_ $\rightarrow$ 검색 + 이커머스 = Multi-modal
2. __Pretrain 및 Downstream task의 성능이 서로 양의 상관관계를 갖는가__
    - 사전학습 오차가 줄면 Transferability는 향상
    - 다른 도메인의 `heterogenerous` 데이터에 대한 전이학습에도 좋은 성능을 보인다
    - 특정 회사 데이터로 학습된 CLUE 모델은 다른 회사에 의미있는 User Representation을 제공한다
3. __사전학습된 Representation이 얼마나 많은 Task를 다룰 수 있는가__
    - Tabular Data를 텍스트로 변환하여 공통된 Semantic Representation을 제공
    - 모든 데이터를 텍스트 형태로 변환함에 따라, 다른 서비스 간의 데이터 형태 불일치를 완화한다
4. __사전학습 모델을 키우면 Generalization 성능이 향상되는가__
5. __성능 향상을 위해 어떤 `factor`를 조절해야 하는가__
    - 실증적으로 `Empirical` 나타나는 Scaling Law
    - 모델 성능은 `model_size, seq_length, batch_size` 등 다양한 factor에 따라 결정된다

## CLUE
공통 사용자 Pool을 갖는 `검색과 이커머스 플랫폼`의 ___사용자 1100만명___ 의 ___500억개의 행동 토큰___ 으로 사전학습된 범용적 User Representation

> 다양한 서비스의 학습 데이터에서 범용적 User Representation을 얻고 통합적 관점을 제공하기
> - 서로 다른 서비스 간의 User Semantics를 어떻게 동일 선상에 둘 것인가? ~~Item의 ID가 아닌~~ ___각 Item의 Text Description을 활용한다___
> - (Item을 Text로 표현할 수 있는) 다른 서비스 및 회사 플랫폼에 User Representation을 제공할 수 있으므로, 범용적이면서 유연하게 User Embedding을 활용할 수 있다.

_Other services and even other company platforms where items can be described with text_

![](https://d3i71xaburhd42.cloudfront.net/7567744a0e23174166575e8d98590967684696b4/2-Figure1-1.png)

|Embedding Type|Type의 의미|Type 예시|
|---|---|---|
|Token Embedding|의미를 갖는 최소 단위|단어|
|Item Embedding|Item을 표현하는 Token의 시퀀스|제품명/설명, 검색 쿼리|
|User Embedding|User를 표현하는 Item의 시퀀스|제품 구매 기록, 검색 기록|

|Symbol|Meaning|Description|
|---|---|---|
|$T_{item}:R^{L\times D_{in}}\rightarrow R^{L\times D_{out}}$|Item Transformer|사용자 행동 로그를 Item의 Feature로 Encoding|
|$T_{service}:R^{S\times D_{out}}\rightarrow R^{S\times D_{out}}$|Service Transformer|Item Embedding 시퀀스를 특정 서비스의 User Embedding으로 Encoding|
|$D_{in}$|`token_embedding_dim`|Token Embedding 차원 수|
|$D_{out}$|`output_dim`|Output 차원 수|
|$L$|`max_token_length`|Item Description (Token 시퀀스) 내 Token 수|
|$S$|`num_items_item_sequence`|Item 시퀀스 내 Item 수|
|$u=[x_{1}, x_{2}, ... x_{S}]$|`user_behavior_log`|사용자가 해당 서비스에서 행동하거나 선택한 Item의 시퀀스|
|$x_{i}\in V^{L}$|`item`|각 Token의 Vocab $V$의 Index로 구성된 Vector|
|$g:V\rightarrow R^{D_{in}}$|`embedding_layer`|Token $\rightarrow$ Embedding Vector|
|$E_{i}\in R^{L\times D_{in}}$|`token_embedding`|Item을 표현하는 Token들의 Embedding으로 구성된 Matrix|
|$h_{i} = \text{MEAN}(T_{item}(E_{i}))$|`item_embedding_vector`|Item에 대한 Embedding|
|$H=[h_{1}\| h_{2} \|...\|h_{S}]$|`item_embedding`|User를 표현하는 Item들의 Embedding으로 구성된 Matrix|
|$z=\text{MEAN}(T_{service}(H))$|`user_embedding_vector`|User에 대한 최종 Embedding|

Contrastive Learning 방식은 CLIP 논문의 loss 정의를 따른다.  
> Contrastive Learning
> - 각 Service task마다 User Representation을 구축
> - 동일 사용자의 User Representation 쌍은 Positive Samples, 아닌 쌍은 Negative Samples로 간주

## Appendix
### Datasets
![](https://d3i71xaburhd42.cloudfront.net/7567744a0e23174166575e8d98590967684696b4/9-Table7-1.png)

### Inter-company-level transfer framework
![](https://d3i71xaburhd42.cloudfront.net/7567744a0e23174166575e8d98590967684696b4/10-Figure6-1.png)

## Miscellaneous
> _Catastrophic forgetting_
> - 새 정보를 학습하는 과정에서 급진적, 불시적으로 이전에 학습한 정보를 잊어먹는 신경망의 경향

