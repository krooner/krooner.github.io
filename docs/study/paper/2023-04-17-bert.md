---
layout: post
title:  "BERT"
parent: Paper
grand_parent: Study
# nav_order: 2
permalink: /docs/study/paper/2023-04-17-bert
date:   2023-04-17 18:00:00 +0900
---
# Pre-training of Deep Bidirectional Transformers for Language Understanding
{: .no_toc }

## 목차
{: .no_toc .text-delta }

1. TOC
{:toc}

## Abstract
> _BERT_, stands for __Bidirectional Encoder Representations from Transformers.__ ... BERT is designed to _pretrain deep bidirectional representations from __unlabeled text__ by __jointly conditioning on both left and right context__ in all layers._

> the __pre-trained BERT model can be fine-tuned__ with just one additional output layer to create SOTA models for a wide range of tasks ... without substantial task-specific architecture modifications.

## Introduction
> _Language model pre-training_ has been shown to be effective for improving many NLP tasks ... There are two existing strategies for __applying pre-trained language representations to downstream tasks: _feature-based_ and _fine-tuning_._

> The feature-based approach, such as ELMo uses task-specific architectures that __include the pre-trained representations as additional features.__ The fine-tuning approach, such as the Generative Pre-trained Transformer __introduces minimal task-specific parameters, and is trained on the downstream tasks__ by simply fine-tuning all pretrained parameters. .. where ___they use unidirectional language models___ to learn general language representations.

> __The major limitation is that standard language models are unidirectional__ ... in OpenAI GPT, ... every token can only attend to previous tokens ... could be very harmful when applying fine-tuning based approaches to token-level tasks ... where ___it is crucial to incorporate context from both directions.___

> BERT alleviates the previously mentioned unidirectionality constraint by using a __"masked language model" (MLM) pre-training objective__, inspired by the Cloze task. MLM __randomly masks some of the tokens from the input__, and the objective is to ___predict the original vocabulary id of the masked word based only on its context.___ ... we also use a _"next sentence prediction" task that jointly pretrains text-pair representations.

> We demonstrate the importance of _bidirecitonal pre-training for language representations._ ... BERT is the first fine-tuning based represetation model that achieves SOTA performance on a large suite of sentence-level and token-level tasks._

## Related Work
> Unsupervised _Feature-based_ Approaches: _Pre-trained word embeddings_ are an integral part of modern NLP systems ... ELMo and its predecessor ... extract _context-sensitive_ features __from a left-to-right and right-to-left language model.__ The contextual representation of each token is __the concatenation of the left-to-right and right-to-left representations.__


> Unsupervised _Fine-tuning_ Approaches: sentence or document encoders which produce contextual token representations have been __pre-trained from unlabeled text and fine-tuned for a supervised downstream task.__ ... OpenAI GPT ... Left-to-right language modeling and auto-encoder objectives have been used for pre-training such models.

## BERT
> There are two steps in our framework: _pre-training_ and _fine-tuning_. During __pre-training, the model is trained on unlabeled data over different pre-training tasks.__ For fine-tuning, the BERT model is __first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.__

### Model Architecture
> __BERT's model archiecture is a multi-layer bidirectional Transformer encoder__ ... we denote the number of layers as $L$, the hidden size as $H$ and the number of self-attention heads as $A$. ... $\textbf{BERT}_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes.

|Hyperparameter|$\textbf{BERT}_{BASE}$|$\textbf{BERT}_{LARGE}$|
|---|---|---|
|$L$|12|24|
|$H$|768|1024|
|$A$|12|16|
|Total parameters|110M|340M|

### Input/Output Representations
> our input representation is able to unambiguously represent __both a single sentence and a pair of sentences in one token sequence.__ ... We use __WordPiece embeddings with a 30,000 token vocabulary.__  _The first token of every sequence is always a special classification token_ `[CLS]`. __The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.__

> Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token `[SEP]`. Second, we add a learned embedding to every token indicating whether it belongs to sentence `A` or sentence `B`. ... For a given token, __its input representation is constructed by _summing_ the corresponding token, segment, and position embeddings.__

|Symbol|Meaning|
|---|---|
|$E$|Input embedding|
|$C\in R^{H}$|Final hidden vector of `[CLS]` token|
|$T_{i}\in R^{H}$|Final hidden vector for the $i$ th input token|

## Pretraining BERT

> ___Masked Langauge Model (MLM)___ : since bidirectional conditioning would allow each word to indirectly "see itself", and the model could trivially predict the target word in a multi-layered context. ... __we simply mask some percentage of the input token at random, and then predict those masked tokens.__ ... _the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary_ ... we mask `15%` of all WordPiece tokens in each sequence at random.

> a downside is that we are creating _a mismatch between pre-training and fine-tuning, since the `[MASK]` token does not appear during fine-tuning._ To mitigate this, __we do not always replace "masked" words with the actual `[MASK]` token.__ ... The training data generator chooses 15% of the token positions at random for prediction. If the $i$-th token is chosen, we replace the $i$-th token with 
> - the `[MASK]` token `80%` of the time 
> - a random token `10%` of the time
> - the unchanged $i$-th token `10% of the time`
>
> Then, $T_{i}$ will be used to predict the original token with cross entropy loss.

> ___Next Sentence Prediction (NSP)___: understanding the _relationship_ between two sentences, which is not directly captured by language modeling. ... we pre-train for a binarized _next sentence prediction_ task ... when choosing the sentences `A` and `B` for each pre-training example, 
> - 50% of the time `B` is the actual next sentence that follows `A` (labeled as $\text{IsNext}$) 
> - 50% of the time it is a random sentence from the corpus (labeled as $\text{NotNext}$).
>
> $C$ is used for next sentence prediction (NSP). ... in prior work, only sentence embeddings are transferred to downstream tasks, where __BERT transfers all parameters to initialize end-task model parameters.__

## Finetuning BERT
> Fine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks by swapping out the appropriate inputs and outputs. ... to independently encode text pairs before applying bidirectional cross attention ... BERT instead uses the ___self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.___

> At the _input_, sentence `A` and `B` from pre-training are analogous ... At the _output_, __the token representations are fed into an output layer for token-level tasks__ such as sequence tagging or QA, and __the `[CLS]` representation is fed into an output layer for classification__ such as entailment or sentiment analysis.